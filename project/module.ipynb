{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try to explore the `PySINDy` package architecture, in order to easily implement our bayesian version of this algorithm with a sparsity inducing prior.\n",
    "\n",
    "We will implement a \"custom\" `optimizer` module, that will implement a Maximum A Posteriori (_MAP_) algorithm over the distribution derived from data and a (possibly sparsity-inducing) prior distribution.\n",
    "The main difference is that this object will retain information over the whole probability distribution for the coefficients $\\boldsymbol{\\xi}$ and not just the best estimates, so that we can evaluate uncertainties over such parameters.\n",
    "\n",
    "The goal of this algorithm is to compute\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\xi}|\\boldsymbol{\\dot{u}},\\boldsymbol{\\Theta}) = P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{\\Theta})P(\\boldsymbol{\\xi})\n",
    "$$\n",
    "\n",
    "Under the assumption that the likelihood of observing $\\boldsymbol{\\dot{u}}$ given a certain coefficients vector $\\boldsymbol{\\xi}$ is a gaussian with mean given by the linear relation:\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{\\Theta}) \\sim \\mathcal{N}(\\boldsymbol{\\Theta}^T\\boldsymbol{\\xi},\\sigma^2)\n",
    "$$\n",
    "\n",
    "\n",
    "And $P(\\boldsymbol{\\xi})$ will be a sparsity inducing prior, so that the original goal of finding the smallest amount of explanatory terms possible is somewhat obtained.\n",
    "\n",
    "### Spike and Slab regression\n",
    "\n",
    "The spike and slab regression is considered to be the golden standard of sparsity inducing priors. The main idea behind it is to build prior that's a \"mixture\" of some smooth other \"slab\" prior (such as some ridge regression gaussian pit) and a delta function (the spike), centered at some point $v$; a latent random variable $Z \\sim Ber(\\theta_i)$ decides which prior to use:\n",
    "\n",
    "$$\n",
    "P(\\xi_i | z_i) = 0 \\sim \\delta(\\xi_i-v) \\\\\n",
    "P(\\xi_i | z_i) = 1 \\sim P_{slab}(\\xi_i) \n",
    "$$\n",
    "\n",
    "So that the prior is the slab function with probability $\\theta_i$ or collapses at $v$ with probability $1-\\theta_i$. Setting $v=0$ corresponds to the sparsity assumption. If we marginalize over $z_i$:\n",
    "\n",
    "$$\n",
    "P(\\xi_i) = \\sum_{z_i=0}^1 P(\\xi_i|z_i)P(z_i) = \\theta_i P_{slab}(\\xi_i) + (1-\\theta_i)\\delta(\\xi_i)\n",
    "$$\n",
    "\n",
    "If we denote with $\\circ$ the Hadamard product (element-wise product), we can express the likelihood of the data given a certain \"masking vector\" $\\boldsymbol{z}$ and a coefficients vector $\\boldsymbol{\\xi}$ as\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{z},\\boldsymbol{\\Theta},\\sigma^2) \\sim \\mathcal{N}(<\\boldsymbol{z}\\circ \\boldsymbol{\\xi},\\Theta>,\\sigma^2)\n",
    "$$\n",
    "\n",
    "And thus, if we simply denote the data as $\\mathcal{D}$, we have the posterior\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{z},\\boldsymbol{\\xi}|\\mathcal{D}) = \\frac{1}{P(\\mathcal{D})} P(\\boldsymbol{z}) P_{slab}(\\boldsymbol{\\xi})\\prod_{\\mathcal{D}}P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{z},\\boldsymbol{\\Theta},\\sigma^2)\n",
    "$$\n",
    "\n",
    "The evidence would need to be computed by integrating over all (infinitely many) possible values of $\\boldsymbol{\\xi}$ and the all the possible ($2^N$) combinations for $\\boldsymbol{z}$; it is clear that an analytical derivation of this posterior distribution is infeasible and a sampling approach will need to be deployed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `BaseOptimizer` class\n",
    "\n",
    "This is the wrapper class for each optimizer algorithm that the package provides; we will build a optimizer module as a subclass of this wrapper. <a href=https://pysindy.readthedocs.io/en/latest/_modules/pysindy/optimizers/base.html#BaseOptimizer>Source code</a> is available on the documentation.\n",
    "\n",
    "#### Bayesian Regression implementation\n",
    "\n",
    "The class will evaluate the best coefficients by performing a Gradient Descent on the posterior distribution, obtained by Bayes theorem with the previously presented likelihood and a prior of choice:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\xi}_{best} = \\argmin_{\\boldsymbol{\\xi}}  \\left [ - P(\\boldsymbol{\\xi}|\\boldsymbol{\\boldsymbol{\\dot{u}}},\\boldsymbol{\\Theta}) \\right ] = \\argmin_{\\boldsymbol{\\xi}} \\left[ N \\log{(\\sigma\\sqrt{2\\pi})}  + \\frac{\\sum_i^N (\\boldsymbol{\\dot{u}} - \\boldsymbol{\\Theta}^T\\boldsymbol{\\xi} )^2}{2\\sigma^2} - \\log{P(\\boldsymbol{\\xi})} \\right ]\n",
    "$$\n",
    "\n",
    "The initial guess for the coefficients will be the result of a OLS algorithm (already provided by the `BaseOptimizer` class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysindy.optimizers import BaseOptimizer\n",
    "import numpy as np\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.linear_model._base import _preprocess_data\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "\n",
    "def _rescale_data(X, y, sample_weight):\n",
    "    \"\"\"Rescale data so as to support sample_weight\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    sample_weight = np.asarray(sample_weight)\n",
    "    if sample_weight.ndim == 0:\n",
    "        sample_weight = np.full(n_samples, sample_weight, dtype=sample_weight.dtype)\n",
    "    sample_weight = np.sqrt(sample_weight)\n",
    "    sw_matrix = sparse.dia_matrix((sample_weight, 0), shape=(n_samples, n_samples))\n",
    "    X = safe_sparse_dot(sw_matrix, X)\n",
    "    y = safe_sparse_dot(sw_matrix, y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class SpikeSlabRegression (BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Bayesian Regression with a Spike and Slab type prior Optimizer.\n",
    "    \n",
    "    Computes the most likely combination of the coefficient vector\n",
    "    and the masking vector using Bayesian inference to compute the \n",
    "    posterior distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_intercept : boolean, optional (default False)\n",
    "        Whether to calculate the intercept for this model. If set to false, no\n",
    "        intercept will be used in calculations.\n",
    "    \n",
    "    return_uncertainty : boolean, optional (default False)\n",
    "        ???? COSA CALCOLIAMO ???\n",
    "\n",
    "    normalize_columns : boolean, optional (default False)\n",
    "        Normalize the columns of x (the SINDy library terms) before regression\n",
    "        by dividing by the L2-norm. Note that the 'normalize' option in sklearn\n",
    "        is deprecated in sklearn versions >= 1.0 and will be removed.\n",
    "\n",
    "    copy_X : boolean, optional (default True)\n",
    "        If True, X will be copied; else, it may be overwritten.\n",
    "\n",
    "    theta_prior : array, shape (n_features), optional (default 0.5 for each feature)\n",
    "        Prior belief about the probability of a coefficient being zero.\n",
    "        (Prior Bernoulli probabilities for the vector z)\n",
    "\n",
    "    initial_guess_z : ....???\n",
    "        HERE OR IN FIT METHOD?\n",
    "\n",
    "    alpha : float, optional (defualt None)\n",
    "        L2 penalization coefficient. This determines the strength of the prior on the coefficients.\n",
    "        If None, no prior will be applied (uniform prior)\n",
    "    \n",
    "    ----------------------------------------???? GRADIENT DESCENT NO\n",
    "\n",
    "    lr : float, optional (default 0.1)\n",
    "        Learning rate for the gradient descent. \n",
    "\n",
    "    tol : float, optional (default 1e-5)\n",
    "        Tolerance used for determining convergence of the optimization\n",
    "        algorithm.\n",
    "\n",
    "    max_iter : int, optional (default 100)\n",
    "        Maximum iterations of the optimization algorithm.\n",
    "    \n",
    "    verbose : boolean, optional (default False)\n",
    "        enables verbose\n",
    "    \n",
    "    sigma : float, optional (default 1) ----> IDEA: DEFAULT TO RESIDUAL SUM OF OLS?\n",
    "        initial guess for the standard eviation of the gaussian distribution for the target value.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    coef_ : array, shape (n_features,) or (n_targets,n_features)\n",
    "        Coefficients vector.\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "\n",
    "    sample(N) :\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_iter=20, \n",
    "        normalize_columns=False, \n",
    "        fit_intercept=False, \n",
    "        initial_guess=None, \n",
    "        copy_X=True,\n",
    "        prior='spikeslab',\n",
    "        lr=0.1,\n",
    "        tol=1e-5,\n",
    "        max_iter=100,\n",
    "        initial_guess_z=None,\n",
    "        theta_prior=None,\n",
    "        alpha=None,\n",
    "        verbose=False\n",
    "        ):\n",
    "\n",
    "        # super() calls a temporary version of the parent class\n",
    "        # this way we pass the init parameters to the class itself via inheritance\n",
    "        # without having to rewrite everything\n",
    "        super().__init__(max_iter, normalize_columns, fit_intercept, initial_guess, copy_X)\n",
    "\n",
    "        self.alpha=alpha\n",
    "        if initial_guess_z is None:\n",
    "            self.initial_guess_z = np.ones_like(initial_guess) # initialize initial guess as all being unmasked\n",
    "        if theta_prior is None:\n",
    "            self.theta_prior = np.ones_like(initial_guess)/2 # 0.5 probability for each\n",
    "        self.prior=prior\n",
    "        self.lr=lr\n",
    "        self.tol=tol\n",
    "        if self.max_iter <= 0:\n",
    "            raise ValueError(\"Max iteration must be > 0\")\n",
    "        self.verbose=verbose\n",
    "\n",
    "    # WE WILL OVERRIDE THE FIT METHOD FROM BaseEstimator SINCE WE WANT DIFFERENT .ind_ attributes\n",
    "\n",
    "    def fit(self, x_, y, sample_weight=None, **reduce_kws):\n",
    "        \"\"\"\n",
    "        Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_ : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "\n",
    "        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
    "            Target values\n",
    "\n",
    "        sample_weight : float or numpy array of shape (n_samples,), optional\n",
    "            Individual weights for each sample\n",
    "\n",
    "        reduce_kws : dict\n",
    "            Optional keyword arguments to pass to the _reduce method\n",
    "            (implemented by subclasses)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------- rescaling part\n",
    "        x_, y = check_X_y(x_, y, accept_sparse=[], y_numeric=True, multi_output=True)\n",
    "\n",
    "        x, y, X_offset, y_offset, X_scale = _preprocess_data(\n",
    "            x_,\n",
    "            y,\n",
    "            fit_intercept=self.fit_intercept,\n",
    "            copy=self.copy_X,\n",
    "            sample_weight=sample_weight,\n",
    "        )\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            x, y = _rescale_data(x, y, sample_weight)\n",
    "\n",
    "        self.iters = 0\n",
    "\n",
    "\n",
    "        # ------------ preparing dimensions, if there is only one target (only one time derivative) then we set it (-1,1) shape\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        coef_shape = (y.shape[1], x.shape[1])\n",
    "        self.ind_ = np.ones(coef_shape, dtype=bool)\n",
    "\n",
    "        # ----------- normalization\n",
    "        self.Theta_ = x # saving original theta\n",
    "        x_normed = np.copy(x)\n",
    "        if self.normalize_columns:\n",
    "            reg = 1 / np.linalg.norm(x, 2, axis=0)\n",
    "            x_normed = x * reg\n",
    "\n",
    "\n",
    "        # ----------- initial guess via ols\n",
    "        if self.initial_guess is None:\n",
    "            self.coef_ = np.linalg.lstsq(x_normed, y, rcond=None)[0].T\n",
    "        else:\n",
    "            if not self.initial_guess.shape == coef_shape:\n",
    "                raise ValueError(\n",
    "                    \"initial_guess shape is incompatible with training data. \"\n",
    "                    f\"Expected: {coef_shape}. Received: {self.initial_guess.shape}.\"\n",
    "                )\n",
    "            self.coef_ = self.initial_guess\n",
    "\n",
    "        self.history_ = [self.coef_]\n",
    "\n",
    "        # WHERE THE MAGIC HAPPENS\n",
    "\n",
    "        self._reduce(x_normed, y, **reduce_kws)\n",
    "        #self.ind_ = np.abs(self.coef_) > 1e-14 # WE WILL SET THIS IN THE REDUCE METHOD, its gonna be the most probable z vector\n",
    "\n",
    "        # Rescale coefficients to original units\n",
    "        if self.normalize_columns:\n",
    "            self.coef_ = np.multiply(reg, self.coef_)\n",
    "            if hasattr(self, \"coef_full_\"):\n",
    "                self.coef_full_ = np.multiply(reg, self.coef_full_)\n",
    "            for i in range(np.shape(self.history_)[0]):\n",
    "                self.history_[i] = np.multiply(reg, self.history_[i])\n",
    "\n",
    "        self._set_intercept(X_offset, y_offset, X_scale)\n",
    "        return self\n",
    "\n",
    "    def _z_prior(self,z):\n",
    "        \"\"\"\n",
    "        Probability of a certain vector z under the prior\n",
    "        Given by vectorized Bernoulli\n",
    "        \"\"\"\n",
    "        return np.prod(self.theta_prior**z * (1 - self.theta_prior)**(1-z))\n",
    "\n",
    "    def _coefs_prior(self,coefs):\n",
    "        if self.alpha is None:\n",
    "            # uniform pseudo prior\n",
    "            return 1\n",
    "        else:\n",
    "            # a gaussian with covariance as a diagonal identity matrix times 1/alpha\n",
    "            return multivariate_normal.pdf(coefs,mean=None,cov=np.eye(len(coefs))/self.alpha)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def _reduce(self,x,y):\n",
    "        \"\"\"\n",
    "        Reduce method to actually perform the minimization.\n",
    "        \"\"\"\n",
    "\n",
    "    def _posterior_unnormalized(self,x,y,coefs,z,sigma):\n",
    "\n",
    "        likelihood = multivariate_normal.pdf()\n",
    "\n",
    "        self._z_prior(z)*self._coef_prior(coefs)*\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b48686ecf5c051869e44bca573c1817bb1844fb32a5df209df0a7813f2e01a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
