{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try to explore the `PySINDy` package architecture, in order to easily implement our bayesian version of this algorithm with a sparsity inducing prior.\n",
    "\n",
    "We will implement a \"custom\" `optimizer` module, that will implement a Maximum A Posteriori (_MAP_) algorithm over the distribution derived from data and a (possibly sparsity-inducing) prior distribution.\n",
    "The main difference is that this object will retain information over the whole probability distribution for the coefficients $\\boldsymbol{\\xi}$ and not just the best estimates, so that we can evaluate uncertainties over such parameters.\n",
    "\n",
    "The goal of this algorithm is to compute\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\xi}|\\boldsymbol{\\dot{u}},\\boldsymbol{\\Theta}) = P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{\\Theta})P(\\boldsymbol{\\xi})\n",
    "$$\n",
    "\n",
    "Under the assumption that the likelihood of observing $\\boldsymbol{\\dot{u}}$ given a certain coefficients vector $\\boldsymbol{\\xi}$ is a gaussian with mean given by the linear relation:\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{\\Theta}) \\sim \\mathcal{N}(\\boldsymbol{\\Theta}^T\\boldsymbol{\\xi},\\sigma^2)\n",
    "$$\n",
    "\n",
    "\n",
    "And $P(\\boldsymbol{\\xi})$ will be a sparsity inducing prior, so that the original goal of finding the smallest amount of explanatory terms possible is somewhat obtained.\n",
    "\n",
    "### Spike and Slab regression\n",
    "\n",
    "The spike and slab regression is considered to be the golden standard of sparsity inducing priors. The main idea behind it is to build prior that's a \"mixture\" of some smooth other \"slab\" prior (such as some ridge regression gaussian pit) and a delta function (the spike), centered at some point $v$; a latent random variable $Z \\sim Ber(\\theta_i)$ decides which prior to use:\n",
    "\n",
    "$$\n",
    "P(\\xi_i | z_i) = 0 \\sim \\delta(\\xi_i-v) \\\\\n",
    "P(\\xi_i | z_i) = 1 \\sim P_{slab}(\\xi_i) \n",
    "$$\n",
    "\n",
    "So that the prior is the slab function with probability $\\theta_i$ or collapses at $v$ with probability $1-\\theta_i$. Setting $v=0$ corresponds to the sparsity assumption. If we marginalize over $z_i$:\n",
    "\n",
    "$$\n",
    "P(\\xi_i) = \\sum_{z_i=0}^1 P(\\xi_i|z_i)P(z_i) = \\theta_i P_{slab}(\\xi_i) + (1-\\theta_i)\\delta(\\xi_i)\n",
    "$$\n",
    "\n",
    "If we denote with $\\circ$ the Hadamard product (element-wise product), we can express the likelihood of the data given a certain \"masking vector\" $\\boldsymbol{z}$ and a coefficients vector $\\boldsymbol{\\xi}$ as\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{z},\\boldsymbol{\\Theta},\\sigma^2) \\sim \\mathcal{N}(<\\boldsymbol{z}\\circ \\boldsymbol{\\xi},\\Theta>,\\sigma^2)\n",
    "$$\n",
    "\n",
    "And thus, if we simply denote the data as $\\mathcal{D}$, we have the posterior\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{z},\\boldsymbol{\\xi}|\\mathcal{D}) = \\frac{1}{P(\\mathcal{D})} P(\\boldsymbol{z}) P_{slab}(\\boldsymbol{\\xi})\\prod_{\\mathcal{D}}P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{z},\\boldsymbol{\\Theta},\\sigma^2)\n",
    "$$\n",
    "\n",
    "The evidence would need to be computed by integrating over all (infinitely many) possible values of $\\boldsymbol{\\xi}$ and the all the possible ($2^N$) combinations for $\\boldsymbol{z}$; it is clear that an analytical derivation of this posterior distribution is infeasible and a sampling approach will need to be deployed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `BaseOptimizer` class\n",
    "\n",
    "This is the wrapper class for each optimizer algorithm that the package provides; we will build a optimizer module as a subclass of this wrapper. <a href=https://pysindy.readthedocs.io/en/latest/_modules/pysindy/optimizers/base.html#BaseOptimizer>Source code</a> is available on the documentation.\n",
    "\n",
    "#### Bayesian Regression implementation\n",
    "\n",
    "The class will evaluate the best coefficients by performing a Gradient Descent on the posterior distribution, obtained by Bayes theorem with the previously presented likelihood and a prior of choice:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\xi}_{best} = \\argmin_{\\boldsymbol{\\xi}}  \\left [ - P(\\boldsymbol{\\xi}|\\boldsymbol{\\boldsymbol{\\dot{u}}},\\boldsymbol{\\Theta}) \\right ] = \\argmin_{\\boldsymbol{\\xi}} \\left[ N \\log{(\\sigma\\sqrt{2\\pi})}  + \\frac{\\sum_i^N (\\boldsymbol{\\dot{u}} - \\boldsymbol{\\Theta}^T\\boldsymbol{\\xi} )^2}{2\\sigma^2} - \\log{P(\\boldsymbol{\\xi})} \\right ]\n",
    "$$\n",
    "\n",
    "The initial guess for the coefficients will be the result of a OLS algorithm (already provided by the `BaseOptimizer` class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysindy.optimizers import BaseOptimizer\n",
    "import numpy as np\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.linear_model._base import _preprocess_data\n",
    "from scipy.stats import beta,gamma,multivariate_normal,binom,mode\n",
    "\n",
    "\n",
    "\n",
    "def _rescale_data(X, y, sample_weight):\n",
    "    \"\"\"Rescale data so as to support sample_weight\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    sample_weight = np.asarray(sample_weight)\n",
    "    if sample_weight.ndim == 0:\n",
    "        sample_weight = np.full(n_samples, sample_weight, dtype=sample_weight.dtype)\n",
    "    sample_weight = np.sqrt(sample_weight)\n",
    "    sw_matrix = sparse.dia_matrix((sample_weight, 0), shape=(n_samples, n_samples))\n",
    "    X = safe_sparse_dot(sw_matrix, X)\n",
    "    y = safe_sparse_dot(sw_matrix, y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class SpikeSlabRegression (BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Bayesian Regression with a Spike and Slab type prior Optimizer.\n",
    "    \n",
    "    Computes the most likely combination of the coefficient vector\n",
    "    and the masking vector using Bayesian inference to compute the \n",
    "    posterior distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_intercept : boolean, optional (default False)\n",
    "        Whether to calculate the intercept for this model. If set to false, no\n",
    "        intercept will be used in calculations.\n",
    "    \n",
    "    return_uncertainty : boolean, optional (default False)\n",
    "        ???? COSA CALCOLIAMO ???\n",
    "\n",
    "    normalize_columns : boolean, optional (default False)\n",
    "        Normalize the columns of x (the SINDy library terms) before regression\n",
    "        by dividing by the L2-norm. Note that the 'normalize' option in sklearn\n",
    "        is deprecated in sklearn versions >= 1.0 and will be removed.\n",
    "\n",
    "    copy_X : boolean, optional (default True)\n",
    "        If True, X will be copied; else, it may be overwritten.\n",
    "\n",
    "    theta_prior : array, shape (n_features), optional (default 0.5 for each feature)\n",
    "        Prior belief about the probability of a coefficient being zero.\n",
    "        (Prior Bernoulli probabilities for the vector z)\n",
    "\n",
    "    initial_guess_z : ....???\n",
    "        HERE OR IN FIT METHOD?\n",
    "\n",
    "    alpha : float, optional (defualt None)\n",
    "        L2 penalization coefficient. This determines the strength of the prior on the coefficients.\n",
    "        If None, no prior will be applied (uniform prior)\n",
    "    \n",
    "\n",
    "    max_iter : int, optional (default 100)\n",
    "        Maximum iterations of the optimization algorithm.\n",
    "    \n",
    "    verbose : boolean, optional (default False)\n",
    "        enables verbose\n",
    "    \n",
    "    sigma : float, optional (default 1) ----> IDEA: DEFAULT TO RESIDUAL SUM OF OLS?\n",
    "        initial guess for the standard eviation of the gaussian distribution for the target value.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    coef_ : array, shape (n_features,) or (n_targets,n_features)\n",
    "        Coefficients vector.\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    Methods\n",
    "    ----------\n",
    "\n",
    "    sample(N) :\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_iter=20, \n",
    "        normalize_columns=False, \n",
    "        fit_intercept=False, \n",
    "        initial_guess=None, \n",
    "        copy_X=True,\n",
    "        tol=1e-5,\n",
    "        # max_iter=100,\n",
    "        alpha=None,\n",
    "        verbose=False\n",
    "        ):\n",
    "\n",
    "        # super() calls a temporary version of the parent class\n",
    "        # this way we pass the init parameters to the class itself via inheritance\n",
    "        # without having to rewrite everything\n",
    "        super().__init__(max_iter, normalize_columns, fit_intercept, initial_guess, copy_X)\n",
    "\n",
    "        self.tol=tol\n",
    "        self.alpha=alpha\n",
    "        if self.max_iter <= 0:\n",
    "            raise ValueError(\"Max iteration must be > 0\")\n",
    "        self.verbose=verbose\n",
    "\n",
    "    # WE WILL OVERRIDE THE FIT METHOD FROM BaseEstimator SINCE WE WANT DIFFERENT .ind_ attributes\n",
    "\n",
    "    def fit(self, x_, y, sample_weight=None, **reduce_kws):\n",
    "        \"\"\"\n",
    "        Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_ : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "\n",
    "        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
    "            Target values\n",
    "\n",
    "        sample_weight : float or numpy array of shape (n_samples,), optional\n",
    "            Individual weights for each sample\n",
    "\n",
    "        reduce_kws : dict\n",
    "            Optional keyword arguments to pass to the _reduce method\n",
    "            (implemented by subclasses)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------- rescaling part\n",
    "        x_, y = check_X_y(x_, y, accept_sparse=[], y_numeric=True, multi_output=True)\n",
    "\n",
    "        x, y, X_offset, y_offset, X_scale = _preprocess_data(\n",
    "            x_,\n",
    "            y,\n",
    "            fit_intercept=self.fit_intercept,\n",
    "            copy=self.copy_X,\n",
    "            sample_weight=sample_weight,\n",
    "        )\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            x, y = _rescale_data(x, y, sample_weight)\n",
    "\n",
    "        self.iters = 0\n",
    "\n",
    "\n",
    "        # ------------ preparing dimensions, if there is only one target (only one time derivative) then we set it (-1,1) shape\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        coef_shape = (y.shape[1], x.shape[1])\n",
    "        self.ind_ = np.ones(coef_shape, dtype=bool)\n",
    "\n",
    "        # ----------- normalization\n",
    "        self.Theta_ = x # saving original theta\n",
    "        x_normed = np.copy(x)\n",
    "        if self.normalize_columns:\n",
    "            reg = 1 / np.linalg.norm(x, 2, axis=0)\n",
    "            x_normed = x * reg\n",
    "\n",
    "        # ---------------------------------------------DO WE NEED THIS? NOT REALLY\n",
    "        # ----------- initial guess via ols\n",
    "        if self.initial_guess is None:\n",
    "            self.coef_ = np.linalg.lstsq(x_normed, y, rcond=None)[0].T\n",
    "        else:\n",
    "            if not self.initial_guess.shape == coef_shape:\n",
    "                raise ValueError(\n",
    "                    \"initial_guess shape is incompatible with training data. \"\n",
    "                    f\"Expected: {coef_shape}. Received: {self.initial_guess.shape}.\"\n",
    "                )\n",
    "            self.coef_ = self.initial_guess\n",
    "\n",
    "\n",
    "        # ---------------???\n",
    "        self.history_ = [self.coef_]\n",
    "\n",
    "\n",
    "\n",
    "        # WHERE THE MAGIC HAPPENS\n",
    "\n",
    "        self._reduce(x_normed, y, **reduce_kws)\n",
    "        #self.ind_ = np.abs(self.coef_) > 1e-14 # WE WILL SET THIS IN THE REDUCE METHOD, its gonna be the most probable z vector\n",
    "\n",
    "        # Rescale coefficients to original units\n",
    "        if self.normalize_columns:\n",
    "            self.coef_ = np.multiply(reg, self.coef_)\n",
    "            if hasattr(self, \"coef_full_\"):\n",
    "                self.coef_full_ = np.multiply(reg, self.coef_full_)\n",
    "            for i in range(np.shape(self.history_)[0]):\n",
    "                self.history_[i] = np.multiply(reg, self.history_[i])\n",
    "\n",
    "        self._set_intercept(X_offset, y_offset, X_scale)\n",
    "        return self\n",
    "        \n",
    "\n",
    "\n",
    "    def sampling(\n",
    "        self,\n",
    "        y, # y is shaped like (n_samples)\n",
    "        X, # X is shaped like (n_samples,n_features)\n",
    "        a1=0.01,\n",
    "        a2=0.01,\n",
    "        theta=0.5,\n",
    "        a=1.,\n",
    "        b=1.,\n",
    "        s=0.5,\n",
    "        chain_samples=6000,\n",
    "        nr_burnin=1500\n",
    "        ):\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # dictionary of empty arrays to store different samples\n",
    "        res = {\n",
    "            \"beta\" : np.empty((chain_samples,n_features)),\n",
    "            \"z\" : np.empty((chain_samples,n_features)),\n",
    "            \"sigma2\" : np.empty(chain_samples),\n",
    "            \"tau2\" : np.empty(chain_samples),\n",
    "            \"theta\" : np.empty(chain_samples)\n",
    "        }\n",
    "\n",
    "        # initialize the masking as ones\n",
    "        res[\"z\"][0] = np.ones(n_features)\n",
    "        # initialize the beta as least square regression\n",
    "        res[\"beta\"][0] = np.linalg.lstsq(X,y,rcond=None)[0]\n",
    "        # initialize the sigma as the variance of the residuals\n",
    "        res[\"sigma2\"][0] = np.var(y - X @ res[\"beta\"][0])\n",
    "        # initialize the tau2 as one and the theta as 0.5\n",
    "        res[\"tau2\"][0] = 1.\n",
    "        res[\"theta\"][0] = 0.5\n",
    "\n",
    "        # compute only once\n",
    "        XtX = X.T @ X\n",
    "        Xty = X.T @ y\n",
    "\n",
    "        # ----------------- BEGIN SAMPLING\n",
    "\n",
    "        for i in range(1,chain_samples):\n",
    "\n",
    "            # lets retrieve the previous values for easier coding\n",
    "            z_prev = res[\"z\"][i-1]\n",
    "            beta_prev = res[\"beta\"][i-1]\n",
    "            sigma2_prev = res[\"sigma2\"][i-1]\n",
    "            tau2_prev = res[\"tau2\"][i-1]\n",
    "            theta_prev = res[\"theta\"][i-1]\n",
    "\n",
    "            # ------------------ LETS GO WITH THE CONDITIONALS\n",
    "\n",
    "            # sample theta from a Beta distribution\n",
    "            theta_new = beta.rvs(a + np.sum(z_prev),b+np.sum(1-z_prev))\n",
    "\n",
    "            # sample sigma2 from an inverse gamma\n",
    "            err = y - X @ beta_prev\n",
    "            scale = 1./(a2 + (err.T @ err)/2)\n",
    "            sigma2_new = 1./gamma.rvs(a1+n_samples/2,scale=scale)\n",
    "\n",
    "            # sample tau2 from an inverse gamma\n",
    "            scale = 1./((s**2)/2 + (beta_prev.T @ beta_prev)/(2*sigma2_new))\n",
    "            tau2_new = 1./gamma.rvs(0.5+0.5*np.sum(z_prev),scale=scale)\n",
    "\n",
    "            # sample new beta from a multivariate gaussian\n",
    "            covariance = np.linalg.inv(XtX/sigma2_new + np.eye(n_features)/(sigma2_new*tau2_new))\n",
    "            mean = covariance @ Xty /sigma2_new # is this right?\n",
    "            beta_new = multivariate_normal.rvs(mean = mean,cov=covariance)\n",
    "\n",
    "            # now we sample the zjs\n",
    "            # in random order\n",
    "            for j in np.random.permutation(n_features):\n",
    "                \n",
    "                # grab the current vector\n",
    "                z0 = z_prev\n",
    "                # set j to zero\n",
    "                z0[j] = 0.\n",
    "                # get the beta_{-j}\n",
    "                bz0 = beta_new * z0\n",
    "\n",
    "                # compute the u variables (one for each sample)\n",
    "                xj = X[:,j] # the jth feature of each sample\n",
    "                u = y - X @ bz0 \n",
    "                cond_var = np.sum(xj**2) + 1./tau2_new\n",
    "\n",
    "                # compute the chance parameter:\n",
    "                # the probability of extracting zj = 0 is prop to (1-theta)\n",
    "                # while of extracting zj=1 is (.....) mess \n",
    "                # computing the logarithm of these (l0 and l1) means that the probability of extracting zj=1 is\n",
    "                # xi = exp(l1)/(exp(l1)+exp(l0))\n",
    "                # we can also write this as\n",
    "                # xi = 1/(1+ exp(l0-l1))\n",
    "                # this way we can check if exp(l0-l1) overflows and just call it xi = 0\n",
    "\n",
    "                l0 = np.log(1-theta_new)\n",
    "                l1 = np.log(theta_new) \\\n",
    "                    - 0.5 * np.log(tau2_new*sigma2_new) \\\n",
    "                    + (np.sum(xj*u)**2)/(2*sigma2_new*cond_var) \\\n",
    "                    + 0.5*np.log(sigma2_new/cond_var)\n",
    "\n",
    "                el0_l1 = np.exp(l0-l1)\n",
    "                if np.isinf(el0_l1):\n",
    "                    xi = 0\n",
    "                else:\n",
    "                    xi = 1/(1+el0_l1)\n",
    "                \n",
    "                # extract the zj\n",
    "                z_prev[j]=binom.rvs(1,xi)\n",
    "\n",
    "            # once we extracted all zj, store them:\n",
    "            z_new = z_prev\n",
    "\n",
    "            # update everything\n",
    "\n",
    "            res[\"z\"][i] = z_new\n",
    "            res[\"beta\"][i] = beta_new\n",
    "            res[\"sigma2\"][i] = sigma2_new\n",
    "            res[\"tau2\"][i] = tau2_new\n",
    "            res[\"theta\"][i] = theta_new\n",
    "\n",
    "        # ---------- END SAMPLING\n",
    "\n",
    "        for k in res.keys():\n",
    "            res[k] = res[k][nr_burnin:]\n",
    "        \n",
    "        return res \n",
    "\n",
    "\n",
    "    def _reduce(self,x,y,chain_samples=10000,burnin_samples=2000,verbose=False,**sampling_kws):\n",
    "        \"\"\"\n",
    "        Reduce method to actually perform the minimization.\n",
    "        This method performs a Gibbs sampling of the joint probability distribution\n",
    "        Under the spike and slab prior method, and will return the coefficients\n",
    "        as the most probable one, given the most probable masking coefficient.\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples, n_features = x.shape\n",
    "        # what if there are multiple targets? i.e. a 3d dynamical system?\n",
    "        n_targets = y.shape[1]\n",
    "        # then we need to perform the regression and find (n_targets) vector of coefficients.\n",
    "        # so the coefficient will be\n",
    "        coef = np.zeros((n_targets,n_features))\n",
    "        ind = np.zeros((n_targets,n_features))\n",
    "        self.samples = []\n",
    "\n",
    "        for i in range(n_targets):\n",
    "\n",
    "            if True: # CHANGE\n",
    "                print(\"Sampling for feature n# {}...\".format(i))\n",
    "            \n",
    "            # KERNEL DENSITY ESTIMATE HERE\n",
    "            # but for now only pick the mean\n",
    "\n",
    "            # we can call y[i] because it's been reshaped to (-1,1) even if n_targets=1\n",
    "            self.samples.append(self.sampling(y[:,i],x,chain_samples=chain_samples,nr_burnin=burnin_samples,**sampling_kws))\n",
    "            coef[i] = np.mean(self.samples[i]['beta'],axis=0)\n",
    "            ind[i] = mode(self.samples[i]['z']) != 0 # setting the .ind_ attribute as a boolean vector\n",
    "\n",
    "\n",
    "        self.coef_ = coef\n",
    "\n",
    "        self.ind_ = ind\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data vector:\n",
      "[[ 1.          1.          1.        ]\n",
      " [ 2.13544974  4.47671622  1.11455735]\n",
      " [ 6.55693295 13.75965756  4.19598482]\n",
      " [16.71629531 27.18403567 26.32175977]\n",
      " [15.3090133   0.99755363 46.71442136]]\n",
      "\n",
      "\n",
      "Time vector:\n",
      "[0.        0.1001001 0.2002002 0.3003003 0.4004004]\n"
     ]
    }
   ],
   "source": [
    "import pysindy as ps\n",
    "\n",
    "# load data\n",
    "r = np.load(\"./data/lorenz_r.npy\")\n",
    "t = np.load(\"./data/lorenz_t.npy\")\n",
    "\n",
    "# r is shaped like (n_points,n_dimensions)\n",
    "print(\"Data vector:\")\n",
    "print(r[:5])\n",
    "\n",
    "# t is time axis\n",
    "print(\"\\n\\nTime vector:\")\n",
    "print(t[:5])\n",
    "\n",
    "feature_names = ['x','y','z'] # just a label for the features, instead of simply using x1,x2....\n",
    "\n",
    "opt=SpikeSlabRegression()\n",
    "\n",
    "model = ps.SINDy(feature_names=feature_names,optimizer=opt) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling for feature n# 0...\n",
      "Sampling for feature n# 1...\n",
      "Sampling for feature n# 2...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SINDy(differentiation_method=FiniteDifference(),\n",
       "      feature_library=PolynomialLibrary(), feature_names=['x', 'y', 'z'],\n",
       "      optimizer=SpikeSlabRegression())"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CI METTE UN CASINO dipende dalla quantità di samples\n",
    "model.fit(r,t=t) # FIT NON HA KWARGS!!!! DIOCANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x)' = 1.262 1 + -16.132 x + 13.959 y + -0.106 z + 0.006 x^2 + 0.011 x y + 0.239 x z + -0.012 y^2 + -0.186 y z\n",
      "(y)' = 13.100 1 + 9.932 x + 4.209 y + -1.541 z + -0.167 x^2 + 0.205 x y + -0.480 x z + -0.068 y^2 + -0.079 y z + 0.040 z^2\n",
      "(z)' = -13.033 1 + -1.369 x + 0.893 y + 1.840 z + 1.616 x^2 + -0.919 x y + 0.031 x z + 0.466 y^2 + -0.023 y z + -0.178 z^2\n"
     ]
    }
   ],
   "source": [
    "model.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(model.optimizer.samples[0]['z'][:,8])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(model.optimizer.ind_)\n",
    "\n",
    "# FORSE DOBBIAMO PRENDERE LA MEDIA DEI BETA PER CUI Z=1 INVECE DI MARGINALIZZARE SU TUTTO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b48686ecf5c051869e44bca573c1817bb1844fb32a5df209df0a7813f2e01a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
