{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing cell\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysindy as ps\n",
    "# numerical differentiation\n",
    "from scipy.misc import derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySINDy\n",
    "\n",
    "We will first deploy the package `PySINDy` developed by the authors to perform the PDE-FIND method on our generated data.\n",
    "\n",
    "The <a href=https://pysindy.readthedocs.io/en/latest/index.html#>documentation</a> provides a basic description of the package's workflow: it revolves around the use of the `SINDy` object that performs the regression task and consists of three basic components, one for each crucial step of the algorithm:\n",
    "\n",
    "1. `differentiation_method`: determines the computation of the derivatives, but it's possible to supply them manually\n",
    "2. `feature_library`: this determines the library the algorithm will use to try and guess the underlying PDE\n",
    "3. `optimizer`: implements the actual sparse regression algorithm\n",
    "\n",
    "The `SINDy` object has a similar syntax to `sklearn`'s model objects (it was written with `sklearn` compatibility in mind) and is capable of using the SINDy results to, for instance, evolve a different initial condition, or to predict derivatives, and much more.\n",
    "\n",
    "Let's use this to try and retrieve the original Lorenz system from the snapshots of its simulated dynamics. First things first, we load up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data vector:\n",
      "[[1.         1.         1.        ]\n",
      " [1.00012952 1.02598928 0.99834857]\n",
      " [1.0005163  1.05196112 0.99672775]\n",
      " [1.00115757 1.07792254 0.99513773]\n",
      " [1.00205076 1.1038803  0.99357873]]\n",
      "\n",
      "\n",
      "Time vector:\n",
      "[0.         0.00100001 0.00200002 0.00300003 0.00400004]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "r = np.load(\"./data/lorenz_r.npy\")\n",
    "t = np.load(\"./data/lorenz_t.npy\")\n",
    "\n",
    "# r is shaped like (n_points,n_dimensions)\n",
    "print(\"Data vector:\")\n",
    "print(r[:5])\n",
    "\n",
    "# t is time axis\n",
    "print(\"\\n\\nTime vector:\")\n",
    "print(t[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we instantiate a `SINDy` model object. \n",
    "\n",
    "In order to do that, we will need to first specify an `optimizer` object that will operate the regression: the <a href=https://pysindy.readthedocs.io/en/latest/api/pysindy.optimizers.html>documentation</a> again provides an exhaustive description of the available optimizer algorithms. For now, we will simply let the model default to STLSQ submodule, which performs a sequentially thresholded least squares algorithm, i.e. minimizes the loss $||y- Xw ||^2 + \\alpha ||w||_2 ^2$ by performing iteratively a least squares regression followed by a mask application that filters out $w$ coefficients whose magnitude lies under a certain threshold. This is the main algorithm suggested in the author's paper and it defaults to a ridge regression (with L2 norm, as opposed to Lasso regression).\n",
    "\n",
    "Then, we need to define the features library, that is the subspace of possible derivatives combinations we wish to explore to find those that are the most informative about our system's evolution. Again, PySINDy provides a variety of different submodules, and the default one that is called when instantiating the model is `PolynomialLibrary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing pieces\n",
    "\n",
    "\n",
    "feature_names = ['x','y','z']\n",
    "\n",
    "model = ps.SINDy(feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDE-FIND implementation\n",
    "\n",
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = np.load(\"./data/lorenz_r.npy\").T\n",
    "t = np.load(\"./data/lorenz_t.npy\")\n",
    "dt = t[1]-t[0] # its evenly spaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use numpy's gradient method that implements second order accurate central differences\n",
    "\n",
    "xdot = np.gradient(x,dt)\n",
    "ydot = np.gradient(y,dt)\n",
    "zdot = np.gradient(z,dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we want to interpolate a linear regression in a space composed of the original \"features\" plus their derivatives and/or non linear combinations. We want to do this three times, one for each equation of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bias','x','y','z','xy','yz','zx','x^2','y^2','z^2']\n",
    "\n",
    "# build dataframe with these columns\n",
    "\n",
    "# initialize vecotr of coefficients xi\n",
    "\n",
    "# ridge regression on them"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b48686ecf5c051869e44bca573c1817bb1844fb32a5df209df0a7813f2e01a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
