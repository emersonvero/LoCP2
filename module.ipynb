{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try to explore the `PySINDy` package architecture, in order to easily implement our bayesian version of this algorithm with a sparsity inducing prior.\n",
    "\n",
    "We will implement a \"custom\" `optimizer` module, that will implement a Maximum A Posteriori (_MAP_) algorithm over the distribution derived from data and a (possibly sparsity-inducing) prior distribution.\n",
    "The main difference is that this object will retain information over the whole probability distribution for the coefficients $\\boldsymbol{\\xi}$ and not just the best estimates, so that we can evaluate uncertainties over such parameters.\n",
    "\n",
    "The goal of this algorithm is to compute\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\xi}|\\boldsymbol{\\dot{u}},\\boldsymbol{\\Theta}) = P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{\\Theta})P(\\boldsymbol{\\xi})\n",
    "$$\n",
    "\n",
    "Under the assumption that the likelihood of observing $\\boldsymbol{\\dot{u}}$ given a certain coefficients vector $\\boldsymbol{\\xi}$ is a gaussian with mean given by the linear relation:\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\dot{u}}|\\boldsymbol{\\xi},\\boldsymbol{\\Theta}) \\sim \\mathcal{N}(\\boldsymbol{\\Theta}^T\\boldsymbol{\\xi},\\sigma^2)\n",
    "$$\n",
    "\n",
    "\n",
    "And $P(\\boldsymbol{\\xi})$ will be a sparsity inducing prior, so that the original goal of finding the smallest amount of explanatory terms possible is somewhat obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `BaseOptimizer` class\n",
    "\n",
    "This is the wrapper class for each optimizer algorithm that the package provides; we will build a optimizer module as a subclass of this wrapper. <a href=https://pysindy.readthedocs.io/en/latest/_modules/pysindy/optimizers/base.html#BaseOptimizer>Source code</a> is available on the documentation.\n",
    "\n",
    "#### Bayesian Regression implementation\n",
    "\n",
    "The class will evaluate the best coefficients by performing a Gradient Descent on the posterior distribution, obtained by Bayes theorem with the previously presented likelihood and a prior of choice:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\xi}_{best} = \\argmin_{\\boldsymbol{\\xi}}  \\left [ - P(\\boldsymbol{\\xi}|\\boldsymbol{\\boldsymbol{\\dot{u}}},\\boldsymbol{\\Theta}) \\right ] = \\argmin_{\\boldsymbol{\\xi}} \\left[ N \\log{(\\sigma\\sqrt{2\\pi})}  + \\frac{\\sum_i^N (\\boldsymbol{\\dot{u}} - \\boldsymbol{\\Theta}^T\\boldsymbol{\\xi} )^2}{2\\sigma^2} - \\log{P(\\boldsymbol{\\xi})} \\right ]\n",
    "$$\n",
    "\n",
    "The initial guess for the coefficients will be the result of a OLS algorithm (already provided by the `BaseOptimizer` class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysindy.optimizers import BaseOptimizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class BayesianRegression (BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Bayesian Regression Optimizer.\n",
    "    \n",
    "    Evaluates the probability distribution over the coefficients w\n",
    "    by assuming a data likelihood for y of a gaussian centered at X @ w,\n",
    "    with a sparsity inducing prior.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_intercept : boolean, optional (default False)\n",
    "        Whether to calculate the intercept for this model. If set to false, no\n",
    "        intercept will be used in calculations.\n",
    "\n",
    "    prior : string or custom function (default ???)\n",
    "        Prior distribution for the coefficients. Defaults to prior inducing???\n",
    "    \n",
    "    return_uncertainty : boolean, optional (default False)\n",
    "        ???? COSA CALCOLIAMO ???\n",
    "\n",
    "    normalize_columns : boolean, optional (default False)\n",
    "        Normalize the columns of x (the SINDy library terms) before regression\n",
    "        by dividing by the L2-norm. Note that the 'normalize' option in sklearn\n",
    "        is deprecated in sklearn versions >= 1.0 and will be removed.\n",
    "\n",
    "    copy_X : boolean, optional (default True)\n",
    "        If True, X will be copied; else, it may be overwritten.\n",
    "    \n",
    "    lr : float, optional (default 0.1)\n",
    "        Learning rate for the gradient descent. \n",
    "\n",
    "    tol : float, optional (default 1e-5)\n",
    "        Tolerance used for determining convergence of the optimization\n",
    "        algorithm.\n",
    "\n",
    "    max_iter : int, optional (default 100)\n",
    "        Maximum iterations of the optimization algorithm.\n",
    "    \n",
    "    verbose : boolean, optional (default False)\n",
    "        enables verbose\n",
    "    \n",
    "    sigma : float, optional (default 1) ----> IDEA: DEFAULT TO RESIDUAL SUM OF OLS?\n",
    "        initial guess for the standard eviation of the gaussian distribution for the target value.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    coef_ : array, shape (n_features,) or (n_targets,n_features)\n",
    "        Coefficients vector.\n",
    "    \n",
    "    std_ : ???\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_iter=20, \n",
    "        normalize_columns=False, \n",
    "        fit_intercept=False, \n",
    "        initial_guess=None, \n",
    "        copy_X=True,\n",
    "        prior='spikeslab',\n",
    "        lr=0.1,\n",
    "        tol=1e-5,\n",
    "        max_iter=100,\n",
    "        verbose=False\n",
    "        ):\n",
    "\n",
    "        # super() calls a temporary version of the parent class\n",
    "        # this way we pass the init parameters to the class itself via inheritance\n",
    "        # without having to rewrite everything\n",
    "        super().__init__(max_iter, normalize_columns, fit_intercept, initial_guess, copy_X)\n",
    "\n",
    "        self.prior=prior\n",
    "        self.lr=lr\n",
    "        self.tol=tol\n",
    "        if self.max_iter <= 0:\n",
    "            raise ValueError(\"Max iteration must be > 0\")\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def log_posterior(self,coefs,sigma,X,y):\n",
    "        \"\"\"\n",
    "        Logarithm of the posterior distribution.\n",
    "        \"\"\"\n",
    "        n_samples,n_features = X.shape\n",
    "\n",
    "\n",
    "        a = -n_samples*np.log(sigma*np.sqrt(2*np.pi))\n",
    "        b =  -np.sum((y - X @ coefs)**2)/(2*sigma**2)\n",
    "        \n",
    "        return a + b + np.log(self.prior(coefs))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b48686ecf5c051869e44bca573c1817bb1844fb32a5df209df0a7813f2e01a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
